<table>
 <thead>
  <tr>
   <th style="text-align:left;"> Stats </th>
   <th style="text-align:left;"> Description </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> afex </td>
   <td style="text-align:left;"> Convenience functions for analyzing factorial experiments using ANOVA or
          mixed models. aov_ez(), aov_car(), and aov_4() allow specification of between,
          within (i.e., repeated-measures), or mixed between-within (i.e., split-plot)
          ANOVAs for data in long format (i.e., one observation per row), aggregating
          multiple observations per individual and cell of the design. mixed() fits mixed
          models using lme4::lmer() and computes p-values for all fixed effects using
          either Kenward-Roger or Satterthwaite approximation for degrees of freedom (LMM
          only), parametric bootstrap (LMMs and GLMMs), or likelihood ratio tests (LMMs
          and GLMMs). afex uses type 3 sums of squares as default (imitating commercial
          statistical software). </td>
  </tr>
  <tr>
   <td style="text-align:left;"> arules </td>
   <td style="text-align:left;"> Provides the infrastructure for representing,
    manipulating and analyzing transaction data and patterns (frequent
    itemsets and association rules). Also provides interfaces to
    C implementations of the association mining algorithms Apriori and Eclat
    by C. Borgelt. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> arulesViz </td>
   <td style="text-align:left;"> Extends package arules with various visualization techniques for association rules and itemsets. The package also includes several interactive visualizations for rule exploration. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> bayesAB </td>
   <td style="text-align:left;"> A suite of functions that allow the user to analyze A/B test
    data in a Bayesian framework. Intended to be a drop-in replacement for
    common frequentist hypothesis test such as the t-test and chi-sq test. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> bayesboot </td>
   <td style="text-align:left;"> Functions for performing the Bayesian bootstrap as introduced by
    Rubin (1981) &lt;doi:10.1214/aos/1176345338&gt; and for summarizing the result.
    The implementation can handle both summary statistics that works on a
    weighted version of the data and summary statistics that works on a
    resampled data set. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> broom </td>
   <td style="text-align:left;"> Convert statistical analysis objects from R into tidy data frames,
    so that they can more easily be combined, reshaped and otherwise processed
    with tools like 'dplyr', 'tidyr' and 'ggplot2'. The package provides three
    S3 generics: tidy, which summarizes a model's statistical findings such as
    coefficients of a regression; augment, which adds columns to the original
    data such as predictions, residuals and cluster assignments; and glance, which
    provides a one-row summary of model-level statistics. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> BTYD </td>
   <td style="text-align:left;"> This package contains functions for data preparation, parameter estimation, scoring, and plotting for the BG/BB, BG/NBD and Pareto/NBD models. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> BTYDplus </td>
   <td style="text-align:left;"> Provides advanced statistical methods to describe and predict customers'
  purchase behavior in a non-contractual setting. It uses historic transaction records to fit a
  probabilistic model, which then allows to compute quantities of managerial interest on a cohort-
  as well as on a customer level (Customer Lifetime Value, Customer Equity, P(alive), etc.). This
  package complements the BTYD package by providing several additional buy-till-you-die models, that
  have been published in the marketing literature, but whose implementation are complex and non-trivial.
  These models are: NBD, MBG/NBD, BG/CNBD-k, MBG/CNBD-k, Pareto/NBD (HB), Pareto/NBD (Abe) and Pareto/GGG. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> car </td>
   <td style="text-align:left;"> Functions and Datasets to Accompany J. Fox and S. Weisberg, 
  An R Companion to Applied Regression, Second Edition, Sage, 2011. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> caret </td>
   <td style="text-align:left;"> Misc functions for training and plotting classification and
    regression models. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> changepoint </td>
   <td style="text-align:left;"> Implements various mainstream and specialised changepoint methods for finding single and multiple changepoints within data.  Many popular non-parametric and frequentist methods are included.  The cpt.mean(), cpt.var(), cpt.meanvar() functions should be your first point of call. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> chunked </td>
   <td style="text-align:left;"> Text data can be processed chunkwise using 'dplyr' commands. These
    are recorded and executed per data chunk, so large files can be processed with
    limited memory using the 'LaF' package. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> class </td>
   <td style="text-align:left;"> Various functions for classification, including k-nearest
  neighbour, Learning Vector Quantization and Self-Organizing Maps. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> cld2 </td>
   <td style="text-align:left;"> Bindings to Google's C++ library Compact Language Detector 2
    (see &lt;https://github.com/cld2owners/cld2#readme&gt; for more information). Probabilistically
    detects over 80 languages in plain text or HTML. For mixed-language input it returns the
    top three detected languages and their approximate proportion of the total classified 
    text bytes (e.g. 80% English and 20% French out of 1000 bytes). There is also a 'cld3'
    package on CRAN which uses a neural network model instead. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> cld3 </td>
   <td style="text-align:left;"> Google's Compact Language Detector 3 is a neural network model for language 
    identification and the successor of 'cld2' (available from CRAN). The algorithm is still
    experimental and takes a novel approach to language detection with different properties
    and outcomes. It can be useful to combine this with the Bayesian classifier results 
    from 'cld2'. See &lt;https://github.com/google/cld3#readme&gt; for more information. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> C50 </td>
   <td style="text-align:left;"> C5.0 decision trees and rule-based models for pattern recognition. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> cleanNLP </td>
   <td style="text-align:left;"> Provides a set of fast tools for converting a textual corpus into a set of normalized
  tables. Users may make use of a Python back end with 'spaCy' &lt;https://spacy.io&gt;
  or the Java back end 'CoreNLP' &lt;http://stanfordnlp.github.io/CoreNLP/&gt;. A minimal back
  end with no external dependencies is also provided. Exposed annotation tasks include
  tokenization, part of speech tagging, named entity recognition, entity linking, sentiment
  analysis, dependency parsing, coreference resolution, and word embeddings. Summary
  statistics regarding token unigram, part of speech tag, and dependency type frequencies
  are also included to assist with analyses. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> cluster </td>
   <td style="text-align:left;"> Methods for Cluster analysis.  Much extended the original from
	Peter Rousseeuw, Anja Struyf and Mia Hubert,
	based on Kaufman and Rousseeuw (1990) &quot;Finding Groups in Data&quot;. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> confinterpret </td>
   <td style="text-align:left;"> Produces descriptive interpretations of confidence intervals.
    Includes (extensible) support for various test types, specified as sets
    of interpretations dependent on where the lower and upper confidence limits
    sit. Provides plotting functions for graphical display of interpretations. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> corrr </td>
   <td style="text-align:left;"> A tool for exploring correlations.
    It makes it possible to easily perform routine tasks when
    exploring correlation matrices such as ignoring the diagonal,
    focusing on the correlations of certain variables against others,
    or rearranging and visualising the matrix in terms of the
    strength of the correlations. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> corrplot </td>
   <td style="text-align:left;"> A graphical display of a correlation matrix or general matrix.
    It also contains some algorithms to do matrix reordering. In addition,
    corrplot is good at details, including choosing color, text labels,
    color labels, layout, etc. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> cshapes </td>
   <td style="text-align:left;"> Package for CShapes, a GIS dataset of country boundaries (1946-today). Includes functions for data extraction and the computation of distance matrices and -lists. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> e1071 </td>
   <td style="text-align:left;"> Functions for latent class analysis, short time Fourier
	     transform, fuzzy clustering, support vector machines,
	     shortest path computation, bagged clustering, naive Bayes
	     classifier, ... </td>
  </tr>
  <tr>
   <td style="text-align:left;"> effects </td>
   <td style="text-align:left;"> Graphical and tabular effect displays, e.g., of interactions, for 
  various statistical models with linear predictors. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> expss </td>
   <td style="text-align:left;"> Package provides tabulation functions with support for 'SPSS'-style 
        labels, multiple / nested banners, weights, multiple-response variables 
        and significance testing. There are facilities for nice output of tables 
        in 'knitr', R notebooks, 'Shiny' and 'Jupyter' notebooks. Proper methods 
        for labelled variables add value labels support to base R functions and to 
        some functions from other packages. Additionally, the package offers 
        useful functions for data processing in marketing research / social 
        surveys - popular data transformation functions from 'SPSS' Statistics 
        ('RECODE', 'COUNT', 'COMPUTE', 'DO IF', etc.) and 'Excel' ('COUNTIF', 
        'VLOOKUP', etc.). Package is intended to help people to move data 
        processing from 'Excel'/'SPSS' to R. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> fBasics </td>
   <td style="text-align:left;"> Environment for teaching 
	&quot;Financial Engineering and Computational Finance&quot;. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> FFTrees </td>
   <td style="text-align:left;"> Create, visualize, and test fast-and-frugal decision trees (FFTs). FFTs are very simple decision trees for
    binary classification problems. FFTs can be preferable to more complex algorithms because they are easy to communicate, require very little information, and are
    robust against overfitting. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> forecast </td>
   <td style="text-align:left;"> Methods and tools for displaying and analysing
             univariate time series forecasts including exponential smoothing
             via state space models and automatic ARIMA modelling. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> forecTheta </td>
   <td style="text-align:left;"> Routines for forecasting univariate time series using Theta Models. Contains several cross-validation routines. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> gap </td>
   <td style="text-align:left;"> It is designed as an integrated package for genetic data
        analysis of both population and family data. Currently, it
        contains functions for sample size calculations of both
        population-based and family-based designs, probability of
        familial disease aggregation, kinship calculation, statistics
        in linkage analysis, and association analysis involving genetic
        markers including haplotype analysis with or without environmental
        covariates. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> gapminder </td>
   <td style="text-align:left;"> An excerpt of the data available at Gapminder.org. For each of 142
    countries, the package provides values for life expectancy, GDP per capita,
    and population, every five years, from 1952 to 2007. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> gbm </td>
   <td style="text-align:left;"> An implementation of extensions to Freund and
        Schapire's AdaBoost algorithm and Friedman's gradient boosting
        machine. Includes regression methods for least squares,
        absolute loss, t-distribution loss, quantile regression,
        logistic, multinomial logistic, Poisson, Cox proportional
        hazards partial likelihood, AdaBoost exponential loss,
        Huberized hinge loss, and Learning to Rank measures
        (LambdaMart). </td>
  </tr>
  <tr>
   <td style="text-align:left;"> glmnet </td>
   <td style="text-align:left;"> Extremely efficient procedures for fitting the entire lasso or elastic-net regularization path for linear regression, logistic and multinomial regression models, Poisson regression and the Cox model. Two recent additions are the multiple-response Gaussian, and the grouped multinomial regression. The algorithm uses cyclical coordinate descent in a path-wise fashion, as described in the paper linked to via the URL below. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> googleLanguageR </td>
   <td style="text-align:left;"> Call 'Google Cloud' machine learning APIs for text and speech tasks.
  Call the 'Cloud Translation' API &lt;https://cloud.google.com/translate/&gt; for detection 
  and translation of text, the 'Natural Language' API &lt;https://cloud.google.com/natural-language/&gt; to 
  analyse text for sentiment, entities or syntax or the 'Cloud Speech' API 
  &lt;https://cloud.google.com/speech/&gt; to transcribe sound files to text. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> hunspell </td>
   <td style="text-align:left;"> A spell checker and morphological analyzer library designed for
    languages with rich morphology and complex word compounding or character
    encoding. The package can check and analyze individual words as well as
    search for incorrect words within a text, latex, html or xml document. Use
    the 'devtools' package to spell check R documentation with 'hunspell'. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> kernlab </td>
   <td style="text-align:left;"> Kernel-based machine learning methods for classification,
        regression, clustering, novelty detection, quantile regression
        and dimensionality reduction.  Among other methods 'kernlab'
        includes Support Vector Machines, Spectral Clustering, Kernel
        PCA, Gaussian Processes and a QP solver. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> koRpus </td>
   <td style="text-align:left;"> A set of tools to analyze texts. Includes, amongst others, functions for automatic language detection, hyphenation,
               several indices of lexical diversity (e.g., type token ratio, HD-D/vocd-D, MTLD) and readability (e.g., Flesch, SMOG,
               LIX, Dale-Chall). Basic import functions for language corpora are also provided, to enable frequency analyses (supports
               Celex and Leipzig Corpora Collection file formats) and measures like tf-idf. Support for additional languages can be
               added on-the-fly or by plugin packages. Note: For full functionality a local installation of TreeTagger is recommended.
               'koRpus' also includes a plugin for the R GUI and IDE RKWard, providing graphical dialogs for its basic features. The
               respective R package 'rkward' cannot be installed directly from a repository, as it is a part of RKWard. To make full
               use of this feature, please install RKWard from &lt;https://rkward.kde.org&gt; (plugins are detected automatically). Due to
               some restrictions on CRAN, the full package sources are only available from the project homepage. To ask for help,
               report bugs, request features, or discuss the development of the package, please subscribe to the koRpus-dev mailing
               list (&lt;http://korpusml.reaktanz.de&gt;). </td>
  </tr>
  <tr>
   <td style="text-align:left;"> lambda.tools </td>
   <td style="text-align:left;"> Provides tools that manipulate and transform data using methods
    and techniques consistent with functional programming. The idea is that
    through the use of these tools, a program can be reasoned about insomuch
    that the implementation can be proven to be equivalent to the mathematical
    model. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> leaflet.minicharts </td>
   <td style="text-align:left;"> Add and modify small charts on an interactive map created with 
    package 'leaflet'. These charts can be used to represent at same time multiple 
    variables on a single map. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> leaps </td>
   <td style="text-align:left;"> Regression subset selection, including exhaustive search. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> lime </td>
   <td style="text-align:left;"> When building complex models, it is often difficult to explain why
    the model should be trusted. While global measures such as accuracy are
    useful, they cannot be used for explaining why a model made a specific
    prediction. 'lime' (a port of the 'lime' 'Python' package) is a method for
    explaining the outcome of black box models by fitting a local model around
    the point in question an perturbations of this point. The approach is
    described in more detail in the article by Ribeiro et al. (2016) 
    &lt;arXiv:1602.04938&gt;. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> liquidSVM </td>
   <td style="text-align:left;"> Support vector machines (SVMs) and related kernel-based learning
    algorithms are a well-known class of machine learning algorithms, for
    non-parametric classification and regression.
    liquidSVM is an implementation of SVMs whose key features are:
    fully integrated hyper-parameter selection,
    extreme speed on both small and large data sets,
    full flexibility for experts, and
    inclusion of a variety of different learning scenarios:
    multi-class classification, ROC, and Neyman-Pearson learning, and
    least-squares, quantile, and expectile regression. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> lmtest </td>
   <td style="text-align:left;"> A collection of tests, data sets, and examples
 for diagnostic checking in linear regression models. Furthermore,
 some generic tools for inference in parametric models are provided. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> margins </td>
   <td style="text-align:left;"> An R port of Stata's 'margins' command, which can be used to
    calculate marginal (or partial) effects from model objects. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Modeler </td>
   <td style="text-align:left;"> Defines classes and methods to learn models and use them
  to predict binary outcomes.  These are generic tools, but we also
  include specific examples for many common classifiers. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> neuralnet </td>
   <td style="text-align:left;"> Training of neural networks using backpropagation,
    resilient backpropagation with (Riedmiller, 1994) or without
    weight backtracking (Riedmiller and Braun, 1993) or the
    modified globally convergent version by Anastasiadis et al.
    (2005). The package allows flexible settings through
    custom-choice of error and activation function. Furthermore,
    the calculation of generalized weights (Intrator O &amp; Intrator
    N, 1993) is implemented. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> nloptr </td>
   <td style="text-align:left;"> nloptr is an R interface to NLopt. NLopt is a free/open-source library for
    nonlinear optimization, providing a common interface for a number of
    different free optimization routines available online as well as original
    implementations of various other algorithms.
    See http://ab-initio.mit.edu/wiki/index.php/NLopt_Introduction for more
    information on the available algorithms. During installation on Unix the
    NLopt code is downloaded and compiled from the NLopt website. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> NMF </td>
   <td style="text-align:left;"> Provides a framework to perform Non-negative Matrix
    Factorization (NMF). The package implements a set of already published algorithms
    and seeding methods, and provides a framework to test, develop and plug
    new/custom algorithms. Most of the built-in algorithms have been optimized
    in C++, and the main interface function provides an easy way of performing
    parallel computations on multicore machines. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> osmdata </td>
   <td style="text-align:left;"> Download and import of 'OpenStreetMap' ('OSM') data as 'sf' or 'sp'
    objects.  'OSM' data are extracted from the 'Overpass' web server and
    processed with very fast 'C++' routines for return to 'R'. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> party </td>
   <td style="text-align:left;"> A computational toolbox for recursive partitioning.
  The core of the package is ctree(), an implementation of
  conditional inference trees which embed tree-structured 
  regression models into a well defined theory of conditional
  inference procedures. This non-parametric class of regression
  trees is applicable to all kinds of regression problems, including
  nominal, ordinal, numeric, censored as well as multivariate response
  variables and arbitrary measurement scales of the covariates. 
  Based on conditional inference trees, cforest() provides an
  implementation of Breiman's random forests. The function mob()
  implements an algorithm for recursive partitioning based on
  parametric models (e.g. linear models, GLMs or survival
  regression) employing parameter instability tests for split
  selection. Extensible functionality for visualizing tree-structured
  regression models is available. The methods are described in
  Hothorn et al. (2006) &lt;doi:10.1198/106186006X133933&gt;,
  Zeileis et al. (2008) &lt;doi:10.1198/106186008X319331&gt; and 
  Strobl et al. (2007) &lt;doi:10.1186/1471-2105-8-25&gt;. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> PerformanceAnalytics </td>
   <td style="text-align:left;"> Collection of econometric functions for
    performance and risk analysis. This package aims to aid
    practitioners and researchers in utilizing the latest
    research in analysis of non-normal return streams.  In
    general, it is most tested on return (rather than
    price) data on a regular scale, but most functions will
    work with irregular return data as well, and increasing
    numbers of functions will work with P&amp;L or price data
    where possible. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> qdapRegex </td>
   <td style="text-align:left;"> A collection of regular expression tools associated with
        the 'qdap' package that may be useful outside of the context of
        discourse analysis. Tools include
        removal/extraction/replacement of abbreviations, dates, dollar
        amounts, email addresses, hash tags, numbers, percentages,
        citations, person tags, phone numbers, times, and zip codes. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> quantmod </td>
   <td style="text-align:left;"> Specify, build, trade, and analyse quantitative financial trading strategies. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> randomForest </td>
   <td style="text-align:left;"> Classification and regression based on a forest of trees
        using random inputs. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> randomForestExplainer </td>
   <td style="text-align:left;"> A set of tools to help explain which variables are most important in a random forests. Various variable importance measures are calculated and visualized in different settings in order to get an idea on how their importance changes depending on our criteria (Hemant Ishwaran and Udaya B. Kogalur and Eiran Z. Gorodeski and Andy J. Minn and Michael S. Lauer (2010) &lt;doi:10.1198/jasa.2009.tm08622&gt;, Leo Breiman (2001) &lt;doi:10.1023/A:1010933404324&gt;). </td>
  </tr>
  <tr>
   <td style="text-align:left;"> robets </td>
   <td style="text-align:left;"> We provide an outlier robust alternative of the function ets() in the 'forecast' package of Hyndman and Khandakar (2008) &lt;DOI:10.18637/jss.v027.i03&gt;. For each method of a class of exponential smoothing variants we made a robust alternative. The class includes methods with a damped trend and/or seasonal components. The robust method is developed by robustifying every aspect of the original exponential smoothing variant. We provide robust forecasting equations, robust initial values, robust smoothing parameter estimation and a robust information criterion. The method is described in more detail in Crevits and Croux (2016) &lt;DOI:10.13140/RG.2.2.11791.18080&gt;. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> rms </td>
   <td style="text-align:left;"> Regression modeling, testing, estimation, validation,
	graphics, prediction, and typesetting by storing enhanced model design
	attributes in the fit.  'rms' is a collection of functions that
	assist with and streamline modeling.  It also contains functions for
	binary and ordinal logistic regression models, ordinal models for
  continuous Y with a variety of distribution families, and the Buckley-James
	multiple regression model for right-censored responses, and implements
	penalized maximum likelihood estimation for logistic and ordinary
	linear models.  'rms' works with almost any regression model, but it
	was especially written to work with binary or ordinal regression
	models, Cox regression, accelerated failure time models,
	ordinary linear models,	the Buckley-James model, generalized least
	squares for serially or spatially correlated observations, generalized
	linear models, and quantile regression. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> rpart </td>
   <td style="text-align:left;"> Recursive partitioning for classification, 
  regression and survival trees.  An implementation of most of the 
  functionality of the 1984 book by Breiman, Friedman, Olshen and Stone. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> rpart.plot </td>
   <td style="text-align:left;"> Plot 'rpart' models. Extends plot.rpart() and text.rpart()
             in the 'rpart' package. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> rpg </td>
   <td style="text-align:left;"> Allows ad hoc queries and reading and
    writing data frames to and from a database. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> rsample </td>
   <td style="text-align:left;"> Classes and functions to create and summarize different types of resampling objects (e.g. bootstrap, cross-validation). </td>
  </tr>
  <tr>
   <td style="text-align:left;"> RTextTools </td>
   <td style="text-align:left;"> RTextTools is a machine learning package for automatic
        text classification that makes it simple for novice users to
        get started with machine learning, while allowing experienced
        users to easily experiment with different settings and
        algorithm combinations. The package includes nine algorithms
        for ensemble classification (svm, slda, boosting, bagging,
        random forests, glmnet, decision trees, neural networks,
        maximum entropy), comprehensive analytics, and thorough
        documentation. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> sas7bdat </td>
   <td style="text-align:left;"> Read SAS files in the sas7bdat data format. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> scanstatistics </td>
   <td style="text-align:left;"> Detection of anomalous space-time clusters using the scan 
    statistics methodology. Focuses on prospective surveillance of data streams, 
    scanning for clusters with ongoing anomalies. Hypothesis testing is made 
    possible by Monte Carlo simulation. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> showtext </td>
   <td style="text-align:left;"> Making it easy to use various types of fonts ('TrueType',
    'OpenType', Type 1, web fonts, etc.) in R graphs, and supporting most output
    formats of R graphics including PNG, PDF and SVG. Text glyphs will be converted
    into polygons or raster images, hence after the plot has been created, it no
    longer relies on the font files. No external software such as 'Ghostscript' is
    needed to use this package. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> SimDesign </td>
   <td style="text-align:left;"> Provides tools to help safely and efficiently organize Monte Carlo simulations in R.
    The package controls the structure and back-end of Monte Carlo simulations
    by utilizing a general generate-analyse-summarise strategy. The functions provided control
    common simulation issues such as re-simulating non-convergent results, support parallel
    back-end and MPI distributed computations, save and restore temporary files,
    aggregate results across independent nodes, and provide native support for debugging. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> SnowballC </td>
   <td style="text-align:left;"> An R interface to the C libstemmer library that implements
  Porter's word stemming algorithm for collapsing words to a common
  root to aid comparison of vocabulary. Currently supported languages are
  Danish, Dutch, English, Finnish, French, German, Hungarian, Italian,
  Norwegian, Portuguese, Romanian, Russian, Spanish, Swedish
  and Turkish. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> stlplus </td>
   <td style="text-align:left;"> Decompose a time series into seasonal, trend, and remainder
    components using an implementation of Seasonal Decomposition of Time
    Series by Loess (STL) that provides several enhancements over the STL
    method in the stats package.  These enhancements include handling missing
    values, providing higher order (quadratic) loess smoothing with automated
    parameter choices, frequency component smoothing beyond the seasonal and
    trend components, and some basic plot methods for diagnostics. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> sugrrants </td>
   <td style="text-align:left;"> Provides 'ggplot2' graphics for analysing time series data. It aims 
    to fit into the 'tidyverse' and grammar of graphics framework for handling 
    temporal data. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> sweep </td>
   <td style="text-align:left;"> Tidies up the forecasting modeling and prediction work flow, 
    extends the 'broom' package 
    with 'sw_tidy', 'sw_glance', 'sw_augment', and 'sw_tidy_decomp' functions 
    for various forecasting models,
    and enables converting 'forecast' objects to 
    &quot;tidy&quot; data frames with 'sw_sweep'. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> textclean </td>
   <td style="text-align:left;"> Tools to clean and process text.  Tools are geared at
        checking for substrings that are not optimal for analysis and
        replacing or removing them with more analysis friendly
        substrings.  For example, emoticons are often used in text but
        not always easily handled by analysis algorithms.  The
        'replace_emoticon' function replaces emoticons with word
        equivalents. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> tibbletime </td>
   <td style="text-align:left;"> Built on top of the 'tibble' package, 'tibbletime' is an extension
  that allows for the creation of time aware tibbles. Some immediate
  advantages of this include: the ability to perform time based subsetting
  on tibbles, quickly summarising and aggregating results by time periods,
  and calling functions similar in spirit to the map family from 'purrr'
  on time based tibbles. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> tictoc </td>
   <td style="text-align:left;"> This package provides the timing functions 'tic' and 'toc' that
    can be nested. One can record all timings while a complex script is
    running, and examine the values later. It is also possible to instrument
    the timing calls with custom callbacks. In addition, this package provides
    class 'Stack', implemented as a vector, and class 'List', implemented as a
    list, both of which support operations 'push', 'pop', 'first', 'last' and
    'clear'. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> tidygraph </td>
   <td style="text-align:left;"> A graph, while not &quot;tidy&quot; in itself, can be thought of as two tidy
    data frames describing node and edge data respectively. 'tidygraph'
    provides an approach to manipulate these two virtual data frames using the
    API defined in the 'dplyr' package, as well as provides tidy interfaces to 
    a lot of common graph algorithms. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> tidytext </td>
   <td style="text-align:left;"> Text mining for word processing and sentiment analysis using
    'dplyr', 'ggplot2', and other tidy tools. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> timetk </td>
   <td style="text-align:left;"> Get the time series index, signature, and summary from time series objects and
    time-based tibbles. Create future time series based on properties of 
    existing time series index.  
    Coerce between time-based tibbles ('tbl') and 'xts', 'zoo', and 'ts'. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> tidyquant </td>
   <td style="text-align:left;"> Bringing financial analysis to the 'tidyverse'. The 'tidyquant' 
    package provides a convenient wrapper to various 'xts', 'zoo', 'quantmod', 'TTR' 
    and 'PerformanceAnalytics' package 
    functions and returns the objects in the tidy 'tibble' format. The main 
    advantage is being able to use quantitative functions with the 'tidyverse'
    functions including 'purrr', 'dplyr', 'tidyr', 'ggplot2', 'lubridate', etc. See 
    the 'tidyquant' website for more information, documentation and examples. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> tm </td>
   <td style="text-align:left;"> A framework for text mining applications within R. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> topicmodels </td>
   <td style="text-align:left;"> Provides an interface to the C code for Latent Dirichlet
	     Allocation (LDA) models and Correlated Topics Models
	     (CTM) by David M. Blei and co-authors and the C++ code
	     for fitting LDA models using Gibbs sampling by Xuan-Hieu
	     Phan and co-authors. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> tseries </td>
   <td style="text-align:left;"> Time series analysis and computational finance. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> TTR </td>
   <td style="text-align:left;"> Functions and data to construct technical trading rules with R. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> udunits2 </td>
   <td style="text-align:left;"> Provides simple bindings to Unidata's udunits library. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> WordR </td>
   <td style="text-align:left;"> Serves for rendering MS Word documents with R inline code and inserting tables and plots. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> wordcloud </td>
   <td style="text-align:left;"> Pretty word clouds. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> wordcloud2 </td>
   <td style="text-align:left;"> A fast visualization tool for creating wordcloud
    by using wordcloud2.js. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> xgboost </td>
   <td style="text-align:left;"> Extreme Gradient Boosting, which is an efficient implementation
    of the gradient boosting framework from Chen &amp; Guestrin (2016) &lt;doi:10.1145/2939672.2939785&gt;.
    This package is its R interface. The package includes efficient linear 
    model solver and tree learning algorithms. The package can automatically 
    do parallel computation on a single machine which could be more than 10 
    times faster than existing gradient boosting packages. It supports
    various objective functions, including regression, classification and ranking.
    The package is made to be extensible, so that users are also allowed to define
    their own objectives easily. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> xts </td>
   <td style="text-align:left;"> Provide for uniform handling of R's different time-based data classes by extending zoo, maximizing native format information preservation and allowing for user level customization and extension, while simplifying cross-class interoperability. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> zoo </td>
   <td style="text-align:left;"> An S3 class with methods for totally ordered indexed
             observations. It is particularly aimed at irregular time series
             of numeric vectors/matrices and factors. zoo's key design goals
             are independence of a particular index/date/time class and
             consistency with ts and base R by providing methods to extend
             standard generics. </td>
  </tr>
  <tr>
   <td style="text-align:left;"> gabrielrvsc/Hdeconometrics </td>
   <td style="text-align:left;"> Set of R functions for high-dimensional econometrics </td>
  </tr>
  <tr>
   <td style="text-align:left;"> AppliedDataSciencePartners/xgboostExplainer </td>
   <td style="text-align:left;"> Explain the XGBoost results </td>
  </tr>
  <tr>
   <td style="text-align:left;"> NA </td>
   <td style="text-align:left;"> NA </td>
  </tr>
  <tr>
   <td style="text-align:left;"> NA </td>
   <td style="text-align:left;"> NA </td>
  </tr>
  <tr>
   <td style="text-align:left;"> NA </td>
   <td style="text-align:left;"> NA </td>
  </tr>
  <tr>
   <td style="text-align:left;"> NA </td>
   <td style="text-align:left;"> NA </td>
  </tr>
  <tr>
   <td style="text-align:left;"> NA </td>
   <td style="text-align:left;"> NA </td>
  </tr>
  <tr>
   <td style="text-align:left;"> NA </td>
   <td style="text-align:left;"> NA </td>
  </tr>
  <tr>
   <td style="text-align:left;"> NA </td>
   <td style="text-align:left;"> NA </td>
  </tr>
</tbody>
</table>